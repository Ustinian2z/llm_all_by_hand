{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d66ff4",
   "metadata": {},
   "source": [
    "### 手撕transformer_decoder架构\n",
    "\n",
    "原版的比较复杂，一般也不会让写。这里的 Decoder 一般指的是 CausalLM，具体变化是少了 encoder 部分的输入，所以也就没有了 encoder and decoder cross attention\n",
    "\n",
    "causal lm: self_attention + FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e8825",
   "metadata": {},
   "source": [
    "1. causalLM decoder 的流程是 input -> self-attention -> FFN\n",
    "\n",
    "2. [self-attention, FFN] 是一个 block，一般会有很多的 block\n",
    "\n",
    "3. FFN 矩阵有两次变化，一次升维度，一次降维度。其中 LLaMA 对于 GPT 的改进还有把 GeLU 变成了 SwishGLU，多了一个矩阵。所以一般升维会从 4h -> 4h * 2 / 3\n",
    "\n",
    "4. 原版的 transformers 用 post-norm, 后面 gpt2, llama 系列用的是 pre-norm。其中 llama 系列一般用 RMSNorm 代替 GPT and transformers decoder 中的 LayerNorm。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fce4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6452e+00, -6.5689e-01,  2.0645e+00,  8.3845e-01,  1.8109e-01,\n",
       "           5.5268e-01, -1.0549e+00, -1.4376e+00,  1.2504e+00,  4.8248e-01,\n",
       "           1.1925e+00,  8.2608e-01, -3.5209e-01, -5.0234e-01, -2.7126e-01,\n",
       "           2.2702e-01, -4.5262e-02,  6.3747e-01, -2.0026e+00,  4.7674e-01,\n",
       "          -8.0082e-01,  4.0579e-01, -1.6028e+00,  1.8945e-01, -2.6150e-01,\n",
       "           2.3285e+00,  1.0221e+00,  4.6685e-01,  2.1390e-01, -3.8437e-01,\n",
       "           5.5431e-01, -1.1759e+00, -8.3214e-02, -8.1166e-01,  9.1685e-01,\n",
       "          -4.6888e-01,  5.5990e-02, -2.7177e+00,  3.0247e-01, -8.8141e-01,\n",
       "          -7.8093e-01, -1.7215e+00, -2.3153e-01,  1.8139e+00,  2.8382e-02,\n",
       "           1.3981e+00, -9.2162e-01,  2.0731e-01,  1.5194e+00, -6.9500e-01,\n",
       "           2.6262e-01,  3.7997e-01,  8.3887e-01,  8.0925e-01, -7.7429e-01,\n",
       "          -1.8555e-01, -1.4461e+00, -7.1366e-01,  2.5071e-01, -8.7361e-01,\n",
       "          -4.9056e-01, -1.0009e+00,  1.3282e+00, -3.2088e-01],\n",
       "         [-4.1449e-01,  2.9160e-01, -5.6991e-01,  2.8905e-01, -4.6320e-01,\n",
       "          -4.9243e-01,  2.0780e-01, -9.8698e-01,  3.0352e+00, -2.7371e-01,\n",
       "           1.4043e+00,  2.6648e-01, -1.9215e-01, -1.5691e+00, -9.2643e-01,\n",
       "           4.0728e-01,  2.0864e+00, -1.4342e+00, -8.4666e-01, -1.1892e+00,\n",
       "           6.4625e-01,  9.8505e-01,  2.3315e-01, -3.2734e-01,  4.0536e-02,\n",
       "          -9.9016e-02,  2.1601e-01,  1.5845e+00, -1.9582e-01, -4.3421e-01,\n",
       "          -9.1855e-01, -1.7028e+00, -1.5391e+00, -8.8132e-01,  1.1233e+00,\n",
       "           1.6527e+00, -1.2055e+00,  5.3937e-01, -6.5513e-01,  1.0051e-01,\n",
       "          -1.0536e+00,  3.8618e-01,  1.2594e+00,  1.5636e+00, -8.9014e-01,\n",
       "          -1.5408e+00,  5.1852e-01,  3.3843e-01, -2.7491e-01, -6.0360e-01,\n",
       "           1.1314e+00, -1.0345e+00,  1.4887e+00,  6.8536e-01,  6.1924e-01,\n",
       "           4.6625e-01, -3.6978e-01,  8.2779e-02,  1.4401e+00, -1.6864e+00,\n",
       "           7.2938e-01, -3.8810e-01, -3.2271e-02, -6.2725e-01],\n",
       "         [ 8.2718e-01, -3.7410e-01, -6.0927e-01, -5.9729e-01,  7.3257e-01,\n",
       "          -5.0147e-01, -1.4801e+00,  1.5604e+00,  2.2255e+00,  6.1600e-01,\n",
       "           1.2166e+00,  1.9508e+00, -4.0611e-01,  4.6802e-01, -4.6430e-01,\n",
       "          -9.6766e-01,  1.5230e+00, -1.2938e+00,  2.2145e-01,  4.5857e-01,\n",
       "          -3.4333e-01, -7.5751e-01, -1.1739e+00, -1.2867e+00,  5.7472e-01,\n",
       "           1.8950e+00, -7.8767e-01, -8.1732e-02,  5.5699e-01,  3.6630e-02,\n",
       "          -8.6272e-01, -1.2119e+00,  2.4457e-01, -1.4569e+00,  8.5143e-01,\n",
       "           1.3375e+00, -1.7921e+00,  8.9707e-01, -6.7737e-01,  7.3274e-01,\n",
       "           4.1292e-01, -7.3819e-01, -8.5741e-01, -1.0449e-01, -1.7352e+00,\n",
       "          -1.0552e+00, -9.3396e-02, -4.0296e-01,  1.1234e+00, -2.5120e-01,\n",
       "          -8.9934e-01, -3.4658e-02, -3.4723e-01,  3.0752e-01,  2.1865e+00,\n",
       "           1.4713e+00, -1.1020e+00, -6.6133e-01,  5.2015e-02, -2.4090e-01,\n",
       "           5.7241e-01,  9.2676e-01,  8.6315e-01, -1.1932e+00],\n",
       "         [ 7.2062e-01, -7.4829e-02,  1.3409e+00,  1.0667e+00,  5.7630e-01,\n",
       "           9.6693e-01, -1.2426e+00, -1.5126e+00,  1.2565e+00, -1.2120e+00,\n",
       "          -8.6129e-01, -4.1670e-01,  7.6151e-01, -2.3387e+00, -1.9979e+00,\n",
       "           6.2696e-01,  5.4413e-03, -9.4334e-01, -1.4069e+00,  9.6485e-03,\n",
       "          -1.2922e+00, -3.5565e-01, -4.9168e-01,  1.4132e+00,  6.7214e-01,\n",
       "           1.5415e+00, -6.5182e-01,  6.6720e-02, -7.8996e-02,  1.0729e+00,\n",
       "          -7.1660e-02,  9.1100e-02,  7.6551e-01,  1.4409e+00,  9.1958e-01,\n",
       "           1.6247e+00, -7.5507e-01,  6.0211e-01, -1.5490e+00, -2.2519e+00,\n",
       "           5.4371e-01, -2.3689e-01, -6.5150e-01,  4.1771e-01, -7.6929e-01,\n",
       "           7.4194e-02,  2.5209e-01, -1.8334e-01,  4.0782e-01,  3.7073e-01,\n",
       "           9.0863e-01, -2.0887e+00, -4.6826e-01, -4.2005e-01,  1.1591e+00,\n",
       "           1.3259e+00,  4.3082e-01,  1.3764e+00,  1.2732e+00, -1.4521e-01,\n",
       "          -1.7234e-01, -2.9967e-01, -6.9718e-01, -4.4446e-01]],\n",
       "\n",
       "        [[ 1.0114e+00,  5.8652e-01, -5.1379e-01, -1.2277e+00, -4.7733e-01,\n",
       "          -6.4127e-01,  4.6986e-01, -5.9111e-01,  4.6345e-01, -8.7919e-01,\n",
       "           3.0469e-01, -1.1811e+00, -2.1289e-01, -7.8148e-01, -9.7034e-01,\n",
       "           2.4500e+00,  7.2804e-01, -1.2411e+00, -1.9015e+00,  6.8281e-01,\n",
       "           9.7339e-01, -4.0402e-01, -7.8831e-02,  9.4580e-01, -8.5803e-01,\n",
       "           1.3857e+00,  4.1671e-01,  1.5657e+00, -8.3705e-01, -9.0615e-02,\n",
       "          -1.5519e+00,  1.3708e-01, -1.5941e+00, -8.7659e-01,  1.4257e+00,\n",
       "           8.4992e-01,  4.9195e-01, -1.3868e+00,  9.4527e-02,  7.5317e-02,\n",
       "          -1.4406e+00, -2.6204e-02,  6.6130e-01, -1.1794e+00, -2.1465e+00,\n",
       "           2.6818e-01,  5.3986e-02,  1.0568e+00,  1.0991e+00, -5.6624e-01,\n",
       "           1.4704e+00, -1.6486e+00,  1.4720e-01, -2.6633e-01, -1.8360e-01,\n",
       "           3.3880e-02, -1.4025e-01,  8.0553e-01,  5.3127e-01, -5.3975e-01,\n",
       "           1.3767e+00,  1.3880e+00,  1.6682e+00,  8.1492e-01],\n",
       "         [ 1.5275e+00, -1.0720e-01, -6.7897e-01,  1.0074e+00, -3.6340e-01,\n",
       "          -1.2098e+00, -2.1766e-01, -1.6086e+00,  7.7389e-01, -1.5633e+00,\n",
       "          -8.8424e-01, -4.0857e-02,  3.8497e-01, -9.4887e-01, -1.7673e+00,\n",
       "           1.3037e+00,  1.5754e+00,  8.7986e-01, -4.2687e-02, -7.6002e-01,\n",
       "           2.9645e-01,  3.8320e-01, -1.4913e+00,  1.7118e+00,  7.8440e-01,\n",
       "          -1.4180e-01,  2.3908e-01,  1.5273e+00, -1.4060e+00, -9.3521e-02,\n",
       "           6.0141e-01, -5.2228e-01, -1.0203e+00,  1.2093e+00, -1.8825e-01,\n",
       "          -5.4723e-01,  4.7536e-01,  8.6341e-01,  1.1072e+00,  9.1085e-01,\n",
       "          -1.6060e+00, -7.5541e-01, -1.6886e-01, -1.0721e+00, -1.5264e+00,\n",
       "           2.1135e-01, -1.0835e+00, -1.6988e-01, -7.4656e-01, -5.3747e-01,\n",
       "           9.1768e-02, -4.8093e-01,  1.7560e+00, -5.6471e-01, -3.8477e-01,\n",
       "           1.4038e+00, -7.9126e-02,  2.0914e+00,  3.5649e-01, -3.0531e-01,\n",
       "          -9.9090e-02,  8.4500e-01,  2.0614e+00, -1.1959e+00],\n",
       "         [ 1.5167e+00, -4.8495e-01, -6.6955e-01,  1.6869e-01, -9.6564e-01,\n",
       "           8.1806e-01, -1.7241e+00, -4.1807e-01,  2.2115e+00,  6.8842e-01,\n",
       "          -4.6506e-01,  3.5958e-01, -1.7476e-01,  5.6023e-01, -1.1892e+00,\n",
       "           5.8270e-01, -3.6000e-01,  1.4492e+00,  8.4592e-01, -2.3718e-01,\n",
       "           7.1061e-01, -6.3497e-01, -5.5449e-01, -3.6047e-01, -1.5988e+00,\n",
       "          -6.4503e-01,  1.7519e+00, -5.8592e-01,  2.8411e-01,  1.2304e+00,\n",
       "          -6.4995e-01, -1.3683e+00,  2.2266e-01, -1.0373e+00,  2.3620e+00,\n",
       "          -4.2430e-01,  1.3872e+00, -8.4579e-01, -9.9089e-01,  2.2765e-01,\n",
       "          -8.5747e-01,  6.1298e-01,  2.1900e-01, -1.1867e-01, -6.3305e-01,\n",
       "           8.9052e-01, -5.6636e-01,  5.3265e-01, -3.0665e-02, -1.2686e+00,\n",
       "           9.3979e-01, -1.4720e+00,  9.3099e-02,  1.0394e+00, -6.2702e-01,\n",
       "           2.0345e+00, -1.0500e+00,  8.8022e-01, -5.2055e-01, -1.6252e+00,\n",
       "           1.7942e+00,  3.9329e-01, -3.4198e-01, -1.3107e+00],\n",
       "         [-2.7291e-01,  6.9947e-01,  8.6601e-01, -8.2780e-01, -1.3620e+00,\n",
       "          -2.0797e-01, -1.5260e+00, -6.6084e-01,  1.7588e-01,  1.0949e+00,\n",
       "           5.5545e-01, -8.2443e-01,  1.4404e+00, -4.0211e-01,  8.1447e-01,\n",
       "          -2.0835e-01,  2.2687e+00, -1.4521e+00,  8.2721e-01,  4.3810e-01,\n",
       "           5.5922e-01, -7.2721e-01,  2.5277e-01, -6.2682e-01,  3.8082e-01,\n",
       "           1.4630e-01,  1.2234e+00,  7.4146e-01, -1.6941e+00,  1.4999e+00,\n",
       "          -5.7546e-01, -2.0829e+00, -1.3253e+00, -8.3669e-01,  2.0583e+00,\n",
       "           2.3909e-01, -1.0772e+00, -4.1657e-01, -9.4604e-01,  1.2955e+00,\n",
       "          -2.9084e-01, -2.5088e-01, -1.5949e-01,  5.1179e-01, -1.2733e+00,\n",
       "           4.0478e-01, -7.3013e-01, -4.0020e-01, -2.2763e-01,  6.5181e-01,\n",
       "           1.6727e-01, -1.9076e+00,  9.7558e-01,  1.8454e+00,  1.1427e+00,\n",
       "          -3.2744e-01, -1.2707e+00,  5.2971e-01,  2.8064e-01, -1.5427e+00,\n",
       "           1.1046e+00, -4.4699e-01,  1.0156e+00,  6.7341e-01]],\n",
       "\n",
       "        [[ 1.0541e+00,  7.1139e-01,  1.3216e+00,  4.2013e-01,  2.7185e-01,\n",
       "           3.1989e-01,  7.1355e-01, -9.6837e-02, -8.5408e-01, -4.3005e-01,\n",
       "           5.6424e-01,  7.4507e-01, -1.1205e+00, -1.8749e+00, -1.7983e+00,\n",
       "          -5.2342e-01, -8.9484e-02,  7.9759e-01, -1.2452e+00, -1.1554e-02,\n",
       "          -1.2524e+00,  1.5331e+00, -6.8108e-01,  1.0399e-01,  8.4041e-01,\n",
       "          -4.2919e-01, -3.3734e-01,  1.2084e+00,  2.0076e-01,  9.8508e-02,\n",
       "          -1.3682e+00, -1.9205e+00, -2.0250e-01, -2.1614e-01, -5.1098e-02,\n",
       "           1.1221e+00,  1.1591e+00, -6.5487e-01,  8.5064e-01, -2.4153e-01,\n",
       "          -1.0442e+00,  1.0176e+00, -1.0546e+00, -1.4838e+00, -1.5468e+00,\n",
       "           5.6959e-01, -1.0372e+00, -1.2234e+00, -3.5021e-01,  1.2915e+00,\n",
       "          -2.1487e-01, -9.8649e-01,  1.2703e+00, -9.1020e-01,  1.6672e+00,\n",
       "          -1.3648e+00, -2.0465e-01,  1.7024e+00,  1.5801e+00, -3.1634e-01,\n",
       "           4.1713e-01,  1.5365e+00,  1.4356e+00,  6.1225e-01],\n",
       "         [ 3.0623e-01,  2.2169e-01,  1.6626e+00,  1.6396e+00,  6.2379e-01,\n",
       "          -3.3217e-01, -1.6970e+00,  2.3440e-01,  1.4697e+00, -4.0159e-01,\n",
       "          -1.2057e+00,  8.5778e-01,  1.6869e+00, -1.2513e+00,  4.1441e-01,\n",
       "           6.7329e-01,  2.2651e+00,  3.6093e-01,  2.9202e-01,  1.0827e+00,\n",
       "          -7.3449e-01,  4.4527e-01, -6.4374e-01,  9.7020e-01, -4.3782e-01,\n",
       "          -8.1552e-01, -3.5465e-01, -4.8204e-01, -1.0409e+00,  4.6508e-01,\n",
       "           6.5766e-01, -1.3109e+00, -1.6759e+00,  1.1433e+00, -6.3392e-01,\n",
       "           5.7087e-01,  6.3287e-01, -1.2004e+00,  2.3342e-01, -1.8821e+00,\n",
       "          -8.5613e-01,  1.5478e-01,  6.9368e-01, -5.4986e-01,  6.7736e-01,\n",
       "           1.0341e+00, -7.4749e-01, -1.2780e-01, -1.0813e+00,  1.4176e+00,\n",
       "          -1.1912e+00, -5.5691e-01,  7.7213e-01, -6.3911e-02,  6.1938e-01,\n",
       "          -1.5044e+00, -1.2580e+00,  4.8619e-02, -1.4092e+00, -1.4133e+00,\n",
       "           1.2578e+00,  5.3104e-01,  1.4573e+00, -7.1396e-01],\n",
       "         [ 1.9839e+00,  9.6207e-01,  1.2893e+00,  1.5997e+00, -1.1320e+00,\n",
       "          -8.1477e-01, -1.0404e+00,  1.3944e-01, -5.8659e-01, -3.5436e-01,\n",
       "           1.6180e+00,  5.8354e-01, -2.3477e-01, -5.5040e-01,  5.6518e-01,\n",
       "          -3.7352e-01, -4.4390e-01, -1.1577e+00,  3.2978e-01,  1.1033e+00,\n",
       "          -1.3089e-01, -1.5450e+00, -7.5809e-01,  1.3493e+00, -8.8997e-01,\n",
       "           1.6156e+00,  8.4212e-01,  1.4188e+00,  3.7750e-01,  1.1188e+00,\n",
       "          -9.0554e-01,  1.1936e-01, -1.1139e+00, -9.9777e-01, -5.2518e-02,\n",
       "          -9.0299e-01,  5.8225e-01, -2.1298e-01, -1.0511e+00,  1.0435e+00,\n",
       "           7.5631e-01,  1.0146e+00,  6.1633e-01, -1.4159e+00,  8.1389e-01,\n",
       "          -3.6490e-01,  8.2110e-01, -1.2836e+00,  9.4691e-01,  1.5496e+00,\n",
       "          -7.4791e-01, -1.7848e+00, -6.5027e-01, -9.0511e-01, -2.9999e-01,\n",
       "           1.7491e-01, -1.7870e+00,  6.5027e-01,  1.1553e+00, -1.8095e+00,\n",
       "           8.2816e-01, -1.9936e-01, -2.6910e-01, -1.2020e+00],\n",
       "         [ 3.6185e-01,  7.5089e-01, -3.4972e-02,  5.4170e-01,  2.5485e-01,\n",
       "           1.7090e-02, -1.5487e+00, -9.0686e-01,  2.1909e+00,  5.0622e-01,\n",
       "           1.7417e-01,  1.3512e+00, -5.1169e-01, -2.2595e+00, -1.3477e+00,\n",
       "           1.3088e+00,  1.1189e+00, -1.2306e+00, -1.0082e+00, -9.6296e-01,\n",
       "           1.1557e+00,  2.3071e-02,  4.8120e-01,  1.0892e-01,  1.7260e+00,\n",
       "           1.4389e-01, -5.7487e-01,  1.3726e-01,  8.7475e-02, -1.4533e-01,\n",
       "          -1.8448e+00, -1.4395e+00, -1.4706e+00,  1.6487e-03,  2.8909e-01,\n",
       "           9.5963e-01, -6.5073e-01, -1.2147e+00, -1.3328e+00, -1.7508e+00,\n",
       "           7.9390e-01,  1.9911e-01,  2.8307e-01, -9.5231e-01,  3.2439e-01,\n",
       "           2.3727e-01, -3.5756e-01,  9.4103e-01,  1.3516e+00,  4.3294e-01,\n",
       "          -6.5287e-01,  1.3626e-01,  5.8490e-01, -5.3583e-01, -5.6053e-01,\n",
       "          -4.8026e-01,  7.8100e-01,  1.5918e+00, -4.4395e-01, -1.8091e+00,\n",
       "           1.2798e+00,  8.1384e-01,  1.0281e+00,  1.5584e+00]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# block / layer\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "        self.dropout = dropout_rate\n",
    "\n",
    "        # layer (mha, ffn)\n",
    "        # mha\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.drop_att = nn.Dropout(self.dropout)\n",
    "        self.att_ln = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "        ## ffn (升维 -> 降维 -> ln)\n",
    "        self.up_proj = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
    "        self.down_proj = nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        self.act_ffn = nn.GELU()\n",
    "        self.drop_ffn = nn.Dropout(0.1) \n",
    "        self.ffn_ln = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
    "\n",
    "    def attention_layer(self, query, key, value, attn_mask = None):\n",
    "        # output shape: (b, s, h)\n",
    "        key = key.transpose(2, 3)\n",
    "        attn_weight = (query @ key) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # causal llm自带的下三角矩阵以及 padding不同序列的 attention_mask\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.tril()\n",
    "            attn_weight = attn_weight.masked_fill(\n",
    "                attn_mask == 0, float(\"-inf\")\n",
    "            )\n",
    "        else:   # 只有下三角矩阵\n",
    "            attn_mask = torch.ones_like(\n",
    "                attn_weight\n",
    "            ).tril()\n",
    "            attn_weight = attn_weight.masked_fill(\n",
    "                attn_mask == 0, float(\"-inf\")\n",
    "            )\n",
    "        \n",
    "        # 先softmax 再dropout！\n",
    "        attn_weight = torch.softmax(attn_weight, dim = -1)\n",
    "        attn_weight = self.drop_att(attn_weight)\n",
    "        # (b, head_num, seq, head_dim)\n",
    "        mid_out = attn_weight @ value\n",
    "\n",
    "        mid_out = mid_out.transpose(1, 2).contiguous()\n",
    "        batch, seq, _, _ = mid_out.size()\n",
    "        mid_out = mid_out.view(batch, seq, -1)  # cat 成 hidden_dim\n",
    "        output = self.o_proj(mid_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def mha(self, X, mask = None):\n",
    "        # (b, s, h) -> (b, head_num, s, head_dim)\n",
    "        batch, seq, _ = X.size()\n",
    "        query = self.q_proj(X).view(batch, seq, self.head_num, -1).transpose(1,2)\n",
    "        key = self.k_proj(X).view(batch, seq, self.head_num, -1).transpose(1,2)\n",
    "        value = self.v_proj(X).view(batch, seq, self.head_num, -1).transpose(1,2)\n",
    "\n",
    "        output = self.attention_layer(query, key, value, mask)\n",
    "\n",
    "        # post norm shape: (b, s, h)\n",
    "        return self.att_ln(X + output)\n",
    "\n",
    "    def ffn(self, X):\n",
    "        up = self.up_proj(X)\n",
    "        up = self.act_ffn(up)\n",
    "        down = self.down_proj(up)\n",
    "\n",
    "        # dropout\n",
    "        down = self.drop_ffn(down)\n",
    "        # post layernorm\n",
    "        return self.ffn_ln(X + down)\n",
    "\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        X = self.mha(X, attention_mask)\n",
    "        X = self.ffn(X)\n",
    "        return X\n",
    "\n",
    "x = torch.rand(3, 4, 64)\n",
    "net = SimpleDecoderLayer(64, 8)     # head_num = head_dim = 8\n",
    "mask = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [1, 1, 1, 1], \n",
    "            [1, 1, 0, 0], \n",
    "            [1, 1, 1, 0]\n",
    "        ]\n",
    "    )   # (3, 4)\n",
    "    .unsqueeze(1)   # (3, 1, 4)\n",
    "    .unsqueeze(2)   # (3, 1, 1, 4)\n",
    "    .repeat(1, 8, 4, 1)\n",
    ")\n",
    "\n",
    "net(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e3dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0721, 0.1170, 0.0497, 0.1347, 0.0463, 0.0856, 0.1799, 0.0394,\n",
       "          0.0957, 0.1127, 0.0316, 0.0354],\n",
       "         [0.0474, 0.0769, 0.0737, 0.0942, 0.2074, 0.0532, 0.0978, 0.0261,\n",
       "          0.0713, 0.1569, 0.0418, 0.0531],\n",
       "         [0.0891, 0.1044, 0.0462, 0.2078, 0.0408, 0.0805, 0.1207, 0.0316,\n",
       "          0.1288, 0.0826, 0.0290, 0.0385],\n",
       "         [0.0602, 0.0867, 0.0329, 0.0656, 0.0986, 0.1495, 0.0727, 0.0480,\n",
       "          0.1367, 0.0928, 0.0611, 0.0951]],\n",
       "\n",
       "        [[0.0247, 0.1308, 0.0865, 0.0783, 0.0267, 0.0344, 0.1504, 0.1370,\n",
       "          0.0460, 0.0662, 0.0612, 0.1579],\n",
       "         [0.0180, 0.0669, 0.1251, 0.0642, 0.0789, 0.1124, 0.1500, 0.1136,\n",
       "          0.0369, 0.0866, 0.0877, 0.0596],\n",
       "         [0.0293, 0.0486, 0.0515, 0.0349, 0.0461, 0.0764, 0.1028, 0.1529,\n",
       "          0.0489, 0.0702, 0.0785, 0.2599],\n",
       "         [0.0574, 0.0947, 0.0114, 0.0568, 0.0471, 0.0574, 0.2577, 0.2192,\n",
       "          0.0193, 0.0587, 0.0272, 0.0932]],\n",
       "\n",
       "        [[0.0243, 0.0746, 0.0758, 0.0594, 0.0618, 0.1802, 0.1556, 0.1516,\n",
       "          0.0272, 0.0731, 0.0746, 0.0419],\n",
       "         [0.0291, 0.1030, 0.0443, 0.0347, 0.0797, 0.0333, 0.1662, 0.2005,\n",
       "          0.0638, 0.0905, 0.0965, 0.0584],\n",
       "         [0.0652, 0.0797, 0.0527, 0.0734, 0.0725, 0.1293, 0.1084, 0.0877,\n",
       "          0.0788, 0.0658, 0.0894, 0.0970],\n",
       "         [0.0506, 0.0668, 0.0937, 0.0371, 0.0941, 0.0648, 0.2582, 0.0368,\n",
       "          0.0234, 0.1595, 0.0774, 0.0376]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.layer_list = nn.ModuleList(\n",
    "            [\n",
    "                SimpleDecoderLayer(64, 8) for i in range(5)\n",
    "            ]\n",
    "        )\n",
    "        self.emb = nn.Embedding(12, 64) # num_embeddings=12, embedding_dim=64\n",
    "        self.out = nn.Linear(64, 12)\n",
    "\n",
    "    def forward(self, X, mask = None):\n",
    "        # X 的初始shape: (batch_size, seq_len)\n",
    "        X = self.emb(X)\n",
    "        for i, layer in enumerate(self.layer_list):\n",
    "            X = layer(X, mask)\n",
    "        print(X.shape)\n",
    "        output = self.out(X)\n",
    "        return torch.softmax(output, dim = -1)\n",
    "    \n",
    "x = torch.randint(low = 0, high = 12, size = (3, 4))\n",
    "net = Decoder()\n",
    "mask = (\n",
    "    torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]])\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .repeat(1, 8, 4, 1)\n",
    ")\n",
    "\n",
    "net(x, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-all-by-hand (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
