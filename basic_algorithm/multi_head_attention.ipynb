{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9146713b",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff099ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c526110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 1.1111],\n",
      "          [0.0000, 1.1111]],\n",
      "\n",
      "         [[0.0000, 1.1111],\n",
      "          [0.0000, 1.1111]],\n",
      "\n",
      "         [[0.0000, 1.1111],\n",
      "          [0.0000, 1.1111]],\n",
      "\n",
      "         [[0.0000, 1.1111],\n",
      "          [0.0000, 1.1111]],\n",
      "\n",
      "         [[0.0000, 1.1111],\n",
      "          [0.0000, 1.1111]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 1.1111]],\n",
      "\n",
      "         [[0.0000, 1.1111],\n",
      "          [0.0000, 1.1111]],\n",
      "\n",
      "         [[0.0000, 1.1111],\n",
      "          [0.0000, 1.1111]]],\n",
      "\n",
      "\n",
      "        [[[   nan,    nan],\n",
      "          [   nan,    nan]],\n",
      "\n",
      "         [[   nan,    nan],\n",
      "          [   nan,    nan]],\n",
      "\n",
      "         [[   nan,    nan],\n",
      "          [   nan,    nan]],\n",
      "\n",
      "         [[   nan,    nan],\n",
      "          [   nan,    nan]],\n",
      "\n",
      "         [[   nan,    nan],\n",
      "          [   nan,    nan]],\n",
      "\n",
      "         [[   nan,    nan],\n",
      "          [   nan,    nan]],\n",
      "\n",
      "         [[   nan,    nan],\n",
      "          [   nan,    nan]],\n",
      "\n",
      "         [[   nan,    nan],\n",
      "          [   nan,    nan]]],\n",
      "\n",
      "\n",
      "        [[[1.1111, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[1.1111, 0.0000],\n",
      "          [1.1111, 0.0000]],\n",
      "\n",
      "         [[1.1111, 0.0000],\n",
      "          [1.1111, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1111, 0.0000]],\n",
      "\n",
      "         [[1.1111, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[1.1111, 0.0000],\n",
      "          [1.1111, 0.0000]],\n",
      "\n",
      "         [[1.1111, 0.0000],\n",
      "          [1.1111, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1111, 0.0000]]]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4018, -0.2847, -0.2598, -0.5817,  0.1536,  0.0125,  0.1009,\n",
       "           0.6344, -0.2779, -0.2702, -0.2061, -0.0138, -0.0454, -0.2967,\n",
       "          -0.1495, -0.2756,  0.1637,  0.2307, -0.0674, -0.3502,  0.4150,\n",
       "          -0.0220,  0.1567,  0.4199,  0.0176,  0.1254,  0.0143, -0.4063,\n",
       "          -0.1950, -0.2313,  0.2778, -0.1599,  0.1987, -0.3715,  0.0199,\n",
       "           0.1280, -0.1462,  0.1232, -0.6659,  0.1592, -0.0554, -0.1185,\n",
       "          -0.5166, -0.0453, -0.0612,  0.2194,  0.6215, -0.3896, -0.0832,\n",
       "          -0.1930, -0.0470, -0.0054, -0.0187,  0.2994,  0.1914, -0.4119,\n",
       "          -0.1261, -0.0128,  0.0223, -0.1768,  0.1012, -0.0023, -0.1958,\n",
       "          -0.6680,  0.1518, -0.1023,  0.1019, -0.2101,  0.2763, -0.2076,\n",
       "          -0.2705, -0.0869,  0.2849,  0.0713,  0.1786, -0.0509, -0.1446,\n",
       "          -0.1729, -0.0984, -0.0397, -0.3444,  0.0494,  0.0979,  0.0952,\n",
       "           0.0536,  0.1949, -0.1778, -0.1036,  0.0476, -0.0790, -0.0107,\n",
       "           0.4252,  0.0278,  0.1853,  0.0620,  0.2046, -0.0389,  0.2397,\n",
       "          -0.0520,  0.3837, -0.0432, -0.0400, -0.1719,  0.5174, -0.7942,\n",
       "           0.1022,  0.3383,  0.1155, -0.2224,  0.4701, -0.0181, -0.0528,\n",
       "           0.3023, -0.1867, -0.3756, -0.2242,  0.0911,  0.1908, -0.3356,\n",
       "           0.2670,  0.1794, -0.2161,  0.1327,  0.0413,  0.0956,  0.3652,\n",
       "          -0.1613,  0.2132],\n",
       "         [-0.2856, -0.6137, -0.4582, -0.6654,  0.1593, -0.1127, -0.1905,\n",
       "           0.4427, -0.3532, -0.5950, -0.2467, -0.2285, -0.3004, -0.2567,\n",
       "          -0.0080, -0.2985,  0.0371,  0.1017,  0.0189, -0.2523,  0.4801,\n",
       "           0.0589,  0.1159,  0.4697, -0.1834, -0.1421, -0.0300, -0.1464,\n",
       "          -0.1229, -0.2640,  0.1624,  0.0063,  0.1595, -0.5555,  0.2262,\n",
       "          -0.0379,  0.1115,  0.0428, -0.8671,  0.3102, -0.2041,  0.0736,\n",
       "          -0.7014, -0.2459,  0.2123,  0.2744,  0.6563, -0.4145, -0.2386,\n",
       "          -0.1448, -0.1237,  0.1302,  0.1311,  0.0546,  0.4264, -0.2990,\n",
       "          -0.0902, -0.2913,  0.1973, -0.0089, -0.0042,  0.0621, -0.1195,\n",
       "          -0.7952,  0.4692, -0.3026, -0.0393, -0.1759,  0.3871, -0.1489,\n",
       "          -0.1145, -0.0819,  0.4097, -0.0253,  0.0094,  0.0013, -0.1427,\n",
       "          -0.2247,  0.0965,  0.0613, -0.4946,  0.1490,  0.0709,  0.2050,\n",
       "           0.0086,  0.3252, -0.3255, -0.1180,  0.0203, -0.1210,  0.0987,\n",
       "           0.2734,  0.2598, -0.0993, -0.0502,  0.0513,  0.1491,  0.1840,\n",
       "          -0.0224,  0.2770, -0.2413, -0.0815, -0.1821,  0.6696, -0.5647,\n",
       "           0.2658,  0.6401,  0.2401, -0.3220,  0.5633,  0.3143, -0.0673,\n",
       "           0.3147, -0.3034, -0.5217, -0.1770,  0.2793,  0.3714, -0.2304,\n",
       "           0.1045,  0.3571, -0.1105,  0.0192,  0.1893,  0.1729,  0.2298,\n",
       "           0.1564,  0.3306]],\n",
       "\n",
       "        [[    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan]],\n",
       "\n",
       "        [[ 0.1016, -0.2716,  0.6657, -0.2174,  0.3024, -0.2329, -0.6097,\n",
       "          -0.1307, -0.0231, -0.8123,  0.0895,  0.3037,  0.4067, -0.4449,\n",
       "          -0.2120, -0.1453,  0.3082,  0.2102, -0.0427,  0.2113, -0.0136,\n",
       "           0.1430,  0.0644, -0.0206,  0.2245,  0.0102, -0.0909, -0.9123,\n",
       "           0.0065,  0.2629, -0.4407,  0.3770, -0.2268, -0.3177, -0.0323,\n",
       "          -0.4988,  0.2362, -0.3922, -0.4571, -0.3582, -0.2478, -0.5844,\n",
       "          -0.1211,  0.3441, -0.3139, -0.0093,  0.0886,  0.2178,  0.0972,\n",
       "          -0.0343,  0.3842, -0.0528, -0.0913,  0.1663, -0.0036, -0.2998,\n",
       "           0.1902, -0.1452, -0.4701, -0.6098, -0.3743, -0.3108,  0.5879,\n",
       "           0.4208, -0.9035, -0.3412, -0.4776,  0.0661,  0.0907,  0.4600,\n",
       "           0.1425,  0.0138, -0.7048,  0.1384, -1.1750, -0.0180,  0.4047,\n",
       "           0.2422,  0.0676,  0.2310,  0.2576, -0.0628,  0.2006, -0.6515,\n",
       "          -0.6932, -0.0964, -0.0540,  0.4603,  0.5815, -0.0022,  0.2063,\n",
       "           0.1631,  0.0696, -0.1451,  0.4301, -0.2065, -0.2536, -0.2732,\n",
       "          -0.1364,  0.3306, -0.5626,  0.2163, -0.3201, -0.4862,  0.5329,\n",
       "          -0.1025, -0.7169,  0.3243, -0.1856,  0.5304, -0.2086,  0.6401,\n",
       "          -0.5781, -0.3755, -0.3083, -1.0254,  0.5770, -0.1500, -0.1204,\n",
       "           0.3617, -0.8569, -0.4906,  0.3509, -0.0857,  0.1177, -0.1264,\n",
       "          -0.4670,  0.2324],\n",
       "         [ 0.0256, -0.2096,  0.4180,  0.0399,  0.2490, -0.5241, -0.4806,\n",
       "          -0.3044,  0.0499, -0.5686,  0.0115,  0.4265,  0.4741, -0.5970,\n",
       "          -0.2643, -0.2279,  0.2202,  0.4162, -0.5154,  0.1606, -0.0834,\n",
       "          -0.1852,  0.1817,  0.1412,  0.5908,  0.3711, -0.0619, -0.8763,\n",
       "           0.0626,  0.3787, -0.1039,  0.2008,  0.0205, -0.0897,  0.1118,\n",
       "          -0.2932,  0.2961, -0.3261, -0.2834, -0.3669, -0.1254, -0.8718,\n",
       "           0.0202,  0.4583, -0.4746,  0.0180, -0.1196,  0.4341, -0.1706,\n",
       "           0.3092,  0.4791, -0.3912,  0.0901,  0.1658, -0.1382, -0.3266,\n",
       "          -0.0115, -0.2716, -0.2007, -0.5562, -0.4617, -0.3813,  0.4172,\n",
       "           0.3892, -0.3114, -0.0254, -0.0525,  0.1965,  0.1270,  0.4877,\n",
       "           0.1008, -0.0047, -0.4283,  0.0230, -0.7217, -0.1447,  0.1392,\n",
       "           0.2735,  0.2728,  0.3732,  0.2687,  0.0645, -0.1431, -0.2531,\n",
       "          -0.6001,  0.0569, -0.2113,  0.7589,  0.7560,  0.3369,  0.0761,\n",
       "          -0.0695, -0.0871, -0.1176,  0.2486, -0.4263,  0.0933, -0.6638,\n",
       "          -0.1467,  0.2186, -0.4968,  0.5020, -0.2489, -0.3848,  0.6015,\n",
       "          -0.1128, -0.6582,  0.1782,  0.0021,  0.2726, -0.3019,  0.7350,\n",
       "          -0.2240, -0.3089, -0.1164, -0.6457,  0.6881, -0.3047, -0.3156,\n",
       "           0.0837, -1.1356, -0.3844,  0.3013, -0.1870, -0.0555,  0.0346,\n",
       "          -0.2512,  0.1122]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "\n",
    "        # shape: (hidden_dim, head_num * head_dim)\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        # X shape: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        Q = self.q_proj(X)  # (batch_size, seq_len, hidden_dim)\n",
    "        K = self.k_proj(X)  # (batch_size, seq_len, hidden_dim)\n",
    "        V = self.v_proj(X)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 我们希望得到 shape (batch_size, head_num, seq_len, head_dim)\n",
    "        q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        k_state = K.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        v_state = V.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_weight = torch.matmul(\n",
    "            q_state, k_state.transpose(-2, -1)  # (batch_size, head_num, head_dim, seq_len)\n",
    "        ) / math.sqrt(self.head_dim) # (batch_size, head_num, seq_len, seq_len)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, float('-inf')\n",
    "            )\n",
    "\n",
    "        attention_weight = torch.softmax(attention_weight, dim=-1)\n",
    "        attention_weight = self.attn_dropout(attention_weight)\n",
    "        print(attention_weight)\n",
    "\n",
    "        output_mid = torch.matmul(\n",
    "            attention_weight, v_state  # (batch_size, head_num, seq_len, head_dim)\n",
    "        )\n",
    "\n",
    "        # 我们要concat成 shape: (batch_size, seq_len, head_num * head_dim)\n",
    "        \n",
    "        output_mid = output_mid.transpose(1, 2).contiguous()\n",
    "        output_mid = output_mid.view(batch_size, seq_len, -1)\n",
    "\n",
    "        output = self.out_proj(output_mid)\n",
    "        return output\n",
    "\n",
    "x = torch.randn(3, 2, 128)\n",
    "\n",
    "attention_mask = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0, 1],\n",
    "            [0, 0],\n",
    "            [1, 0],\n",
    "        ]\n",
    "    )   # shape: (3, 2)\n",
    "    .unsqueeze(1)   # shape: (3, 1, 2)\n",
    "    .unsqueeze(2)   # shape: (3, 1, 1, 2)\n",
    "    .expand(3, 8, 2, 2) # shape: (3, 8, 2, 2) 这里是广播机制, (batch_size, head_num, seq_len, seq_len)\n",
    ")\n",
    "\n",
    "net = MultiHeadAttention(128, 8) # head_dim = 16\n",
    "net(x, attention_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-all-by-hand (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
