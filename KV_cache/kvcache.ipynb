{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f190749",
   "metadata": {},
   "source": [
    "### 无kv-cache生成代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b03930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "\n",
      "Generation_0 input_ids shape torch.Size([1, 5]):\n",
      "input_ids tensor([[12, 81, 92, 52, 65]]):\n",
      "logits shape torch.Size([1, 5, 100])\n",
      "idx_next:  tensor([66])\n",
      "\n",
      "Generation_1 input_ids shape torch.Size([1, 6]):\n",
      "input_ids tensor([[12, 81, 92, 52, 65, 66]]):\n",
      "logits shape torch.Size([1, 6, 100])\n",
      "idx_next:  tensor([66])\n",
      "\n",
      "Generation_2 input_ids shape torch.Size([1, 7]):\n",
      "input_ids tensor([[12, 81, 92, 52, 65, 66, 66]]):\n",
      "logits shape torch.Size([1, 7, 100])\n",
      "idx_next:  tensor([93])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "相当于每次生成output的时候都要全量过一遍idx, 然后取最后一个的argmax作为idx\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from transformers import LlamaModel, LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size = 100,           # 词汇表大小\n",
    "    hidden_size = 256,          # 隐藏层维度\n",
    "    intermediate_size = 512,    # 前馈网络中间层维度\n",
    "    num_hidden_layers = 2,      # Transformer 层数\n",
    "    num_attention_heads = 4,    # 注意力头的数量\n",
    "    num_key_value_heads = 4,    # 键/值对的头数量（用于 GQA）\n",
    ")\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "X = torch.randint(0, 100, (1, 5))\n",
    "print(X.shape)\n",
    "\n",
    "idx = {}\n",
    "idx['input_ids'] = X\n",
    "for i in range(3):\n",
    "    print(f\"\\nGeneration_{i} input_ids shape {idx['input_ids'].shape}:\")\n",
    "    print(f\"input_ids {idx['input_ids']}:\")\n",
    "    output = model(**idx)\n",
    "    print(f\"logits shape {output['logits'].shape}\")\n",
    "    logits = output['logits'][:, -1, :]\n",
    "    idx_next = torch.argmax(logits, dim=-1)\n",
    "    print('idx_next: ', idx_next)\n",
    "    # 注意这里是unsqueeze(1)，因为idx_next已经有一个[]了\n",
    "    idx['input_ids'] = torch.cat((idx['input_ids'], idx_next.unsqueeze(1)), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c439341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 0 step input_shape: torch.Size([4, 10])：\n",
      "K shape:  torch.Size([4, 10, 128])\n",
      "V shape:  torch.Size([4, 10, 128])\n",
      "attn shape:  torch.Size([4, 10, 10])\n",
      "torch.Size([4, 10, 128])\n",
      "next_token.shape: torch.Size([4])\n",
      "\n",
      "Generation 1 step input_shape: torch.Size([4, 1])：\n",
      "K shape:  torch.Size([4, 11, 128])\n",
      "V shape:  torch.Size([4, 11, 128])\n",
      "attn shape:  torch.Size([4, 1, 11])\n",
      "torch.Size([4, 1, 128])\n",
      "next_token.shape: torch.Size([4])\n",
      "\n",
      "Generation 2 step input_shape: torch.Size([4, 1])：\n",
      "K shape:  torch.Size([4, 12, 128])\n",
      "V shape:  torch.Size([4, 12, 128])\n",
      "attn shape:  torch.Size([4, 1, 12])\n",
      "torch.Size([4, 1, 128])\n",
      "next_token.shape: torch.Size([4])\n",
      "\n",
      "Generation 3 step input_shape: torch.Size([4, 1])：\n",
      "K shape:  torch.Size([4, 13, 128])\n",
      "V shape:  torch.Size([4, 13, 128])\n",
      "attn shape:  torch.Size([4, 1, 13])\n",
      "torch.Size([4, 1, 128])\n",
      "next_token.shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from transformers import LlamaModel, LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "D = 128       # single head dim\n",
    "V = 64        # vocab size\n",
    "B = 4         # batch size\n",
    "L = 10        # seq len  \n",
    "\n",
    "class kv_cache(torch.nn.Module):\n",
    "    def __init__(self, D, V):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.embedding = torch.nn.Embedding(V, D)\n",
    "        self.Wq = torch.nn.Linear(D, D)\n",
    "        self.Wk = torch.nn.Linear(D, D)\n",
    "        self.Wv = torch.nn.Linear(D, D)\n",
    "        self.lm_head = torch.nn.Linear(D, V)\n",
    "        self.cache_k = self.cache_v = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)   # (B, L, V) -> (B, L, D)\n",
    "        Q, K, V = self.Wq(X), self.Wk(X), self.Wv(X)   # (B, L, D)\n",
    "        # 单层，每次增加 2 x batch_size x L x D x 4/2 bits的内存\n",
    "        if self.cache_k is None:\n",
    "            self.cache_k = K\n",
    "            self.cache_v = V\n",
    "        else:\n",
    "            self.cache_k = torch.cat([self.cache_k, K], dim= 1)   # (B, L_cache + L, D)\n",
    "            self.cache_v = torch.cat([self.cache_v, V], dim= 1)   # (B, L_cache + L, D)\n",
    "            K = self.cache_k\n",
    "            V = self.cache_v\n",
    "        print('K shape: ', K.shape)\n",
    "        print('V shape: ', V.shape)\n",
    "\n",
    "        attn = Q @ K.transpose(-1, -2) / (self.D ** 0.5)    # (B, 1, L+L_cache)\n",
    "        print('attn shape: ', attn.shape)\n",
    "        output = attn @ V       # (B, 1, D)\n",
    "        print(output.shape)\n",
    "        output = self.lm_head(output)   # (B, L, V)\n",
    "        return output\n",
    "\n",
    "model = kv_cache(D, V)\n",
    "X = torch.randint(0, V, (B, L))\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"\\nGeneration {i} step input_shape: {X.shape}：\")\n",
    "    output = model.forward(X) \n",
    "    next_token = torch.argmax(F.softmax(output, dim = -1),dim=-1)[:,-1]\n",
    "    print('next_token.shape:', next_token.shape)\n",
    "    X = next_token.unsqueeze(1)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_byhand3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
