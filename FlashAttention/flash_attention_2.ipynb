{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74b49a2",
   "metadata": {},
   "source": [
    "### 用这个，这个简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79480cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------O0 = attn( Q0, KV[0])---------\n",
      "tensor([[[[ 1.3982e+00,  7.5875e-01,  5.6159e-01,  9.9421e-05],\n",
      "          [ 8.9726e-01,  6.7523e-01,  6.1780e-01, -6.7770e-01],\n",
      "          [ 2.3564e+00,  1.0980e+00,  9.9736e-01, -1.4894e-01]]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "-----------O0 = attn( Q0, KV[1])---------\n",
      "tensor([[[[ 1.9457,  2.4848, -0.3509, -4.2035],\n",
      "          [ 1.2030, -0.7594,  0.5988, -0.2114],\n",
      "          [ 0.0331,  1.4902, -0.0575, -2.3302]]]], grad_fn=<AddBackward0>)\n",
      "-----------O1 = attn( Q1, KV[0])---------\n",
      "tensor([[[[1.7440, 0.5494, 0.4743, 0.5904],\n",
      "          [0.6914, 0.2655, 0.1599, 0.5026],\n",
      "          [2.0992, 0.7122, 0.7146, 0.4859]]]], grad_fn=<AddBackward0>)\n",
      "-----------O1 = attn( Q1, KV[1])---------\n",
      "tensor([[[[ 1.8868,  0.7840,  0.4164,  0.4586],\n",
      "          [ 1.8137, -0.1804, -0.2169, -0.1105],\n",
      "          [ 2.9275,  1.8271,  0.3610, -0.6856]]]], grad_fn=<AddBackward0>)\n",
      "O: tensor([[[[ 0.5985,  0.7643, -0.1079, -1.2929],\n",
      "          [ 0.4139, -0.2612,  0.2060, -0.0727],\n",
      "          [ 0.0249,  1.1203, -0.0432, -1.7518],\n",
      "          [ 1.6464,  0.6841,  0.3634,  0.4002],\n",
      "          [ 0.5904, -0.0587, -0.0706, -0.0360],\n",
      "          [ 1.2085,  0.7543,  0.1490, -0.2830]]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "Q_BLOCK_SIZE = 3\n",
    "KV_BLOCK_SIZE = 4\n",
    "NEG_INF = 1e-10\n",
    "EPSILON = 1e-10\n",
    "Q_LEN = 6\n",
    "K_LEN = 8\n",
    "# 注意这个和flash attention 1 的顺序相反\n",
    "Tr = Q_LEN // Q_BLOCK_SIZE      # 外层循环\n",
    "Tc = K_LEN // KV_BLOCK_SIZE     # 内层循环\n",
    "\n",
    "Q = torch.randn(1, 1, Q_LEN, 4, requires_grad = True).to(device='cpu')\n",
    "K = torch.randn(1, 1, K_LEN, 4, requires_grad = True).to(device='cpu')\n",
    "V = torch.randn(1, 1, K_LEN, 4, requires_grad = True).to(device='cpu')\n",
    "O = torch.zeros_like(Q, requires_grad = True)           # O[i]：最终输出向量（对应第 i 个 query token）\n",
    "l = torch.zeros(Q.shape[:-1]).unsqueeze(-1)             # l[i]：当前累积的 softmax 分母\n",
    "m = torch.ones(Q.shape[:-1]).unsqueeze(-1) * NEG_INF    # m[i]：当前累积的最大 attention score\n",
    "\n",
    "# 竖着切\n",
    "Q_blocks = torch.split(Q, Q_BLOCK_SIZE, dim = 2)\n",
    "K_blocks = torch.split(K, KV_BLOCK_SIZE, dim = 2)\n",
    "V_blocks = torch.split(V, KV_BLOCK_SIZE, dim = 2)\n",
    "O_blocks = list(torch.split(O, Q_BLOCK_SIZE, dim=2))\n",
    "l_blocks = list(torch.split(l, Q_BLOCK_SIZE, dim=2))\n",
    "m_blocks = list(torch.split(m, Q_BLOCK_SIZE, dim=2))\n",
    "\n",
    "# start with Q\n",
    "for i in range(Tr):\n",
    "    Qi = Q_blocks[i]          # 当前 query block\n",
    "    Oi = O_blocks[i]\n",
    "    li = l_blocks[i]\n",
    "    mi = m_blocks[i]\n",
    "\n",
    "    for j in range(Tc):\n",
    "        # if j > i:\n",
    "        #     continue    # 忽略mask\n",
    "        Kj = K_blocks[j]\n",
    "        Vj = V_blocks[j]\n",
    "\n",
    "        Sij = Qi @ Kj.transpose(-1, -2)\n",
    "        m_block_ij, _ = torch.max(Sij, dim=-1, keepdim=True)\n",
    "        mi_new = torch.maximum(mi, m_block_ij)\n",
    "        P_ij_hat = torch.exp(Sij - mi_new)\n",
    "        l_block_ij = torch.sum(P_ij_hat, dim=-1, keepdim=True) + EPSILON\n",
    "\n",
    "        li_new = torch.exp(mi - mi_new) * li + l_block_ij\n",
    "        Oi = torch.exp(mi - mi_new) * Oi + P_ij_hat @ Vj\n",
    "\n",
    "        li = li_new\n",
    "        mi = mi_new\n",
    "        print(f'-----------O{i} = attn( Q{i}, KV[{j}])---------')\n",
    "        print(Oi)\n",
    "    # 这个相当于之前每次内部循环都要除一个li_new，但这次只最后除\n",
    "    O_blocks[i] = Oi / li_new   # 最后做scaled\n",
    "    l_blocks[i] = li_new\n",
    "    m_blocks[i] = mi_new\n",
    "\n",
    "\n",
    "O = torch.cat(O_blocks, dim=2)\n",
    "l = torch.cat(l_blocks, dim=2)\n",
    "m = torch.cat(m_blocks, dim=2)\n",
    "\n",
    "print(\"O:\", O)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-all-by-hand (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
