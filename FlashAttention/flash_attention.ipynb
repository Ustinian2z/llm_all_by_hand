{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c8b033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([ 0.1438,  0.8986, -1.4740,  1.1223, -1.6086,  1.5352])\n",
      "online softmax a: tensor([0.0982, 0.2090, 0.0195, 0.2613, 0.0170, 0.3949])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# online SoftMax 2-pass\n",
    "import torch\n",
    "\n",
    "N = 6\n",
    "m = torch.tensor(-1000.0)\n",
    "d = 0\n",
    "x = torch.randn(N)\n",
    "a = torch.zeros(N)\n",
    "print('x:', x)\n",
    "\n",
    "for i in range(N):\n",
    "    m_pre = m\n",
    "    m = torch.max(m, x[i])\n",
    "    d = d * (m_pre - m).exp() + (x[i] - m).exp()\n",
    "    \n",
    "for i in range(N):\n",
    "    a[i] = (x[i]-m).exp() / d\n",
    "    \n",
    "print('online softmax a:',a)\n",
    "print(torch.sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae837c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Attn : Q0xK0---------\n",
      "O blocks 0: tensor([[[[ 0.9302, -0.4377,  1.2176, -0.1538],\n",
      "          [-1.5067,  0.5400, -1.0797,  0.4081],\n",
      "          [ 1.8763, -0.8075,  2.3340, -0.0710]]]], grad_fn=<AddBackward0>)\n",
      "O blocks 1: tensor([[[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]], grad_fn=<SplitBackward0>)\n",
      "\n",
      "\n",
      "-----------Attn : Q1xK0---------\n",
      "O blocks 0: tensor([[[[ 0.9302, -0.4377,  1.2176, -0.1538],\n",
      "          [-1.5067,  0.5400, -1.0797,  0.4081],\n",
      "          [ 1.8763, -0.8075,  2.3340, -0.0710]]]], grad_fn=<AddBackward0>)\n",
      "O blocks 1: tensor([[[[ 2.0871, -0.9031,  2.5503, -0.0987],\n",
      "          [-1.1344,  0.4068, -0.8468,  0.1686],\n",
      "          [-0.2861,  0.4992,  0.0705,  0.2249]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "-----------Attn : Q0xK1---------\n",
      "O blocks 0: tensor([[[[ 0.4608, -0.2396,  0.5418, -0.1609],\n",
      "          [-1.4419,  0.5208, -1.0398,  0.3814],\n",
      "          [ 0.7820,  0.7656,  0.6603, -0.7068]]]], grad_fn=<AddBackward0>)\n",
      "O blocks 1: tensor([[[[ 2.0871, -0.9031,  2.5503, -0.0987],\n",
      "          [-1.1344,  0.4068, -0.8468,  0.1686],\n",
      "          [-0.2861,  0.4992,  0.0705,  0.2249]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "-----------Attn : Q1xK1---------\n",
      "O blocks 0: tensor([[[[ 0.4608, -0.2396,  0.5418, -0.1609],\n",
      "          [-1.4419,  0.5208, -1.0398,  0.3814],\n",
      "          [ 0.7820,  0.7656,  0.6603, -0.7068]]]], grad_fn=<AddBackward0>)\n",
      "O blocks 1: tensor([[[[ 1.7897, -0.7024,  2.1298, -0.1405],\n",
      "          [-0.9893,  0.2885, -0.7447,  0.1441],\n",
      "          [-0.2847,  0.4908,  0.0677,  0.2226]]]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "tensor([[[[ 0.4608, -0.2396,  0.5418, -0.1609],\n",
      "          [-1.4419,  0.5208, -1.0398,  0.3814],\n",
      "          [ 0.7820,  0.7656,  0.6603, -0.7068],\n",
      "          [ 1.7897, -0.7024,  2.1298, -0.1405],\n",
      "          [-0.9893,  0.2885, -0.7447,  0.1441],\n",
      "          [-0.2847,  0.4908,  0.0677,  0.2226]]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# flash attention \n",
    "\n",
    "NEG_INF = -1e10     # 表示负无穷，用于初始化最大值\n",
    "EPSILON = 1e-10     # 防止除零\n",
    "\n",
    "Q_LEN = 6\n",
    "K_LEN = 8\n",
    "Q_BLOCK_SIZE = 3\n",
    "KV_BLOCK_SIZE = 4\n",
    "\n",
    "Tr = Q_LEN // Q_BLOCK_SIZE      # 内层循环\n",
    "Tc = K_LEN // KV_BLOCK_SIZE     # 外层循环\n",
    "\n",
    "Q = torch.randn(1, 1, Q_LEN, 4, requires_grad = True).to(device='cpu')\n",
    "K = torch.randn(1, 1, K_LEN, 4, requires_grad = True).to(device='cpu')\n",
    "V = torch.randn(1, 1, K_LEN, 4, requires_grad = True).to(device='cpu')\n",
    "O = torch.zeros_like(Q, requires_grad = True)   # O[i]：最终输出向量（对应第 i 个 query token）\n",
    "l = torch.zeros(Q.shape[:-1]).unsqueeze(-1)   # l[i]：当前累积的 softmax 分母\n",
    "m = torch.ones(Q.shape[:-1]).unsqueeze(-1) * NEG_INF   # m[i]：当前累积的最大 attention score\n",
    "\n",
    "Q_blocks = torch.split(Q, Q_BLOCK_SIZE, dim=2)\n",
    "K_blocks = torch.split(K, KV_BLOCK_SIZE, dim=2)\n",
    "V_blocks = torch.split(V, KV_BLOCK_SIZE, dim=2)\n",
    "O_blocks = list(torch.split(O, Q_BLOCK_SIZE, dim=2))\n",
    "l_blocks = list(torch.split(l, Q_BLOCK_SIZE, dim=2))\n",
    "m_blocks = list(torch.split(m, Q_BLOCK_SIZE, dim=2))\n",
    "\n",
    "for j in range(Tc):\n",
    "    Kj = K_blocks[j]    # (1, 1, 4, 4)\n",
    "    Vj = V_blocks[j]    # (1, 1, 4, 4)\n",
    "    for i in range(Tr):\n",
    "        Qi = Q_blocks[i]    # (1, 1, 3, 4)\n",
    "        Oi = O_blocks[i]    # (1, 1, 3, 4)\n",
    "        li = l_blocks[i]    # (1, 1, 3, 1)\n",
    "        mi = m_blocks[i]    # (1, 1, 3, 1)\n",
    "\n",
    "        S_ij = Qi @ Kj.transpose(-1, -2)    # (1, 1, 3, 4)\n",
    "        # 这个是沿着最后一维求最大，即每一行遍历过去取每一行中列的最大值\n",
    "        # shape: (1, 1, 3, 1)\n",
    "        m_block_ij, _ = torch.max(S_ij, dim=-1, keepdim=True)   # torch.max 返回两个值，第一个是最大值，第二个是索引\n",
    "        P_ij = torch.exp(S_ij - m_block_ij)  # (1, 1, 3, 4)\n",
    "        l_block_ij = torch.sum(P_ij, dim=-1, keepdim=True)   # (1, 1, 3, 1)\n",
    "        mi_new = torch.maximum(m_block_ij, mi)\n",
    "        P_ij_Vj = P_ij @ Vj  # (1, 1, 3, 4)\n",
    "\n",
    "        li_new = torch.exp(mi - mi_new) * li + torch.exp(m_block_ij - mi_new) * l_block_ij  # (1, 1, 3, 1)\n",
    "        # shape: (1, 1, 3, 4)\n",
    "        O_blocks[i] = (li / li_new) * torch.exp(mi - mi_new) * Oi + (torch.exp(m_block_ij - mi_new) / li_new) * P_ij_Vj\n",
    "        \n",
    "        print(f'-----------Attn : Q{i}xK{j}---------')\n",
    "        print('O blocks 0:', O_blocks[0])\n",
    "        print('O blocks 1:', O_blocks[1])\n",
    "        print('\\n')\n",
    "\n",
    "        l_blocks[i] = li_new\n",
    "        m_blocks[i] = mi_new\n",
    "\n",
    "O = torch.cat(O_blocks, dim = 2)\n",
    "l = torch.cat(l_blocks, dim = 2)\n",
    "m = torch.cat(m_blocks, dim = 2)\n",
    "\n",
    "print(O)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-all-by-hand (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
