{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c93cb2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d960b96850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df09eb5",
   "metadata": {},
   "source": [
    "### 0. 设置Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dff7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 512   # 文本的最大长度 max_seq\n",
    "    batch_size: int = 12\n",
    "    n_layer: int = 8\n",
    "    n_head: int = 8\n",
    "    # 设置成一样为了tie_embedding_weight，共享词向量层（embedding layer）和输出层（output layer）的权重矩阵\n",
    "    n_embd: int = 256   # 也是 hidden_dim/hidden_size的数值\n",
    "    hidden_dim: int = n_embd\n",
    "    dropout: float = 0.1 \n",
    "    head_size: int = n_embd // n_head\n",
    "    # vocab_size\n",
    "    # gpt2 的官方的tokenizer\n",
    "    vocab_size: int = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4644b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOEConfig:\n",
    "    def __init__(self, \n",
    "                 hidden_dim = 256, \n",
    "                 expert_number = 8, \n",
    "                 top_k = 2, \n",
    "                 shared_experts_numbers=2):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.expert_number = expert_number\n",
    "        self.top_k = top_k\n",
    "        self.shared_experts_number = shared_experts_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7398311",
   "metadata": {},
   "source": [
    "### 1. Multihead_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7825434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. single head attention\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.head_size = config.head_size\n",
    "        self.query = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        self.key = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        self.value = nn.Linear(config.hidden_dim, config.head_size)\n",
    "\n",
    "        # 新的写法 attention_mask 通过 register_buffer 注册\n",
    "        # 不用计算梯度，节约内存显存，速度更快\n",
    "        self.register_buffer(\n",
    "            \"attention_mask\",\n",
    "            torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size)\n",
    "            )\n",
    "        )   # 注意这里是一个block_size x block_size的矩阵，用的时候要用[:seq_len, :seq_len]截取一部分\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        weight = q @ k.transpose(-2, -1)\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0,\n",
    "            float(\"-inf\")\n",
    "        )\n",
    "        # 注意计算weight的时候除以根号d_k\n",
    "        weight = F.softmax(weight / math.sqrt(self.head_size), dim = -1)\n",
    "        weight = self.dropout(weight)\n",
    "        \n",
    "        output = weight @ v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07976833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config)\n",
    "                for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat(\n",
    "            [h(x) for h in self.heads],\n",
    "            dim = -1\n",
    "        )\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b7aa4",
   "metadata": {},
   "source": [
    "### 2. MOE架构替换FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d31b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicExpert(nn.Module):\n",
    "    # 一个 Expert 可以是一个最简单的， linear 层即可\n",
    "    # 也可以是 MLP 层\n",
    "    # 也可以是 更复杂的 MLP 层（active function 设置为 swiglu）\n",
    "    def __init__(self, feature_in, feature_out):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(feature_in, 4 * feature_in),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * feature_in, feature_out),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ffn(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b45a099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOERouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_dim, config.expert_number)\n",
    "        # 后面只是选后面 topk个专家\n",
    "        self.expert_number = config.expert_number\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 假设 expert_num = 8, top_k = 2\n",
    "        router_logits = self.gate(x)   # (batch_size * seq_len, expert_number)\n",
    "\n",
    "        # 计算每个专家的概率\n",
    "        router_probs = F.softmax(router_logits, dim = -1, dtype = torch.float)\n",
    "\n",
    "        # router_weights 的 ij 就是第 i 个 token 的 top_j 的专家的权重, 至于专家是哪个 indice 就是指出哪几个专家属于该token的topk\n",
    "        # top_k 是可以反向传播的 \n",
    "        router_weights, selected_experts_indices = torch.topk(\n",
    "            router_probs,\n",
    "            self.top_k,\n",
    "            dim = -1\n",
    "        )   # router_weight, selected_experts_indices 的shape都是 (batch_size * seq_len, top_k)\n",
    "\n",
    "        # 再次softmax一下，重新归一化\n",
    "        router_weights = router_weights / router_weights.sum(\n",
    "            dim = -1, keepdim = True\n",
    "        )           # (batch_size * seq_len, top_k)\n",
    "        router_weights = router_weights.to(x.dtype)\n",
    "\n",
    "        expert_mask = F.one_hot(\n",
    "            selected_experts_indices,\n",
    "            num_classes = self.expert_number,\n",
    "        )   # 输出 (batch_size, * seq_len, top_k, expert_number)\n",
    "\n",
    "        expert_mask = expert_mask.permute(2, 1, 0)  # 希望shape (expert_number, top_k, batch_size * seq_len)\n",
    "\n",
    "        return router_logits, router_weights, selected_experts_indices, expert_mask\n",
    "        \n",
    "        \n",
    "        # router_logits shape (batch_size * seq_len, expert_number)\n",
    "        # router_weights shape (batch_size * seq_len, top_k)\n",
    "        # selected_experts_indices shape (batch_size * seq_len, top_k)\n",
    "        # expert_mask shape (expert_number, top_k, batch_size * seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a85fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.top_k = config.top_k\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.expert_number = config.expert_number\n",
    "\n",
    "        # 初始化专家\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                BasicExpert(\n",
    "                    config.hidden_dim,\n",
    "                    config.hidden_dim\n",
    "                )   for _ in range(config.expert_number)\n",
    "            ]\n",
    "        )\n",
    "        self.router = MOERouter(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "\n",
    "        # token 维度计算, x reshape (batch_size * seq_len, hidden_dim)\n",
    "        hidden_states = x.view(-1, hidden_dim)\n",
    "\n",
    "        # 做相关专家计算\n",
    "        router_logits, router_weights, selected_experts_indices, expert_masks = self.router(hidden_states)\n",
    "\n",
    "        # expert_masks shape (expert_number, top_k, batch_size * seq_len)\n",
    "        # 最终 final_hidden_states 是 (batch_size * seq_len, hidden_dim)\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * seq_len, hidden_dim),\n",
    "            dtype = hidden_states.dtype,\n",
    "            device = hidden_states.device\n",
    "        )\n",
    "\n",
    "        # 遍历专家，把选中该专家的token的hidden_states加入final_hidden_states中\n",
    "        # 如 expert_0 可能有 100 个token选中了，token总数是batch * seq_len\n",
    "        for expert_idx in range(self.expert_number):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "\n",
    "            # expert_masks (expert_num, top_k, batch_size * seq_len)\n",
    "            # current_expert_mask (top_k, batch_size * seq_len)\n",
    "            current_expert_mask = expert_masks[expert_idx]\n",
    "\n",
    "            # top_idx 是 0 or 1 如果假设top_k=2，表示当前 token 是作为当前专家的top1 or top2\n",
    "            # token_x 是 token 在 batch_size * seq_len 的索引位置, 如 b * s = 8，那么就是 0-7\n",
    "            # top_idx 和 token_x 都是一个1维的值，有一个一一对应的关系！\n",
    "            # top_idx为了选expert的哪个weight， token_x 是为了选 hidden_states （到底是哪个token）\n",
    "            top_idx, token_x = torch.where(\n",
    "                current_expert_mask\n",
    "            )\n",
    "            # hidden_states shape (batch_size * seq_len, hidden_dim)\n",
    "            # unsqueeze的 shape (1, batch_size * seq_len, hidden_dim)\n",
    "\n",
    "            # current_state shape (selected_token_number, hidden_dim)\n",
    "            current_state = hidden_states.unsqueeze(0)[:, token_x, :].reshape(-1, hidden_dim)\n",
    "\n",
    "            current_state = expert_layer(current_state)\n",
    "\n",
    "            # current_token_router_weight shape (selected_token_number, )\n",
    "            # token_x, top_idx 是两个长度相同的一维索引张量，PyTorch 会按位置一一对应提取元素，最终形成一个长度等于索引数量的一维张量。\n",
    "            current_token_router_weight = router_weights[token_x, top_idx]\n",
    "            # shape (selected_token_number, 1)\n",
    "            # 虽然这里要知道哪个专家是该token的top_k要查indices，但其实已经得到了该专家的weight，具体是哪个专家就不需要在这一步搞了\n",
    "            current_token_router_weight = current_token_router_weight.unsqueeze(-1)\n",
    "            \n",
    "            # current_state (selected_token_number, hidden_dim) 再* (selected_token_number, 1)广播\n",
    "            current_hidden_states = current_state * current_token_router_weight\n",
    "\n",
    "            # index_add_加下划线代表原地操作\n",
    "            final_hidden_states.index_add_(\n",
    "                dim = 0,    # 沿着哪个维度进行索引和累加\n",
    "                index = token_x,  # 要累加token索引\n",
    "                source = current_hidden_states.to(hidden_states.dtype),   # 提供累加值的源张量\n",
    "            )\n",
    "        # 把 final_hidden_states 还原\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # router_logits shape (batch_size * seq_len, expert_number)，是为了算loss用的\n",
    "        return final_hidden_states, router_logits   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5967d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedExpertMOE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.routed_experts_moe = SparseMOE(config)\n",
    "        self.shared_experts = nn.ModuleList(\n",
    "            [\n",
    "                BasicExpert(self.config.hidden_dim, \n",
    "                            self.config.hidden_dim)\n",
    "                        for _ in range(self.config.shared_experts_number)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (b, s, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "\n",
    "        # concat要先unsqueeze一下，stack可以直接创建新的维度\n",
    "        shared_experts_output_list = [\n",
    "            expert(x) for expert in self.shared_experts\n",
    "        ]\n",
    "        shared_expert_output = torch.stack(\n",
    "            shared_experts_output_list,\n",
    "            dim = 0\n",
    "        )   # shape (shared_experts_number,batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # shape (b, s, hidden_dim)\n",
    "        shared_expert_out = shared_expert_output.sum(dim=0, keepdim=False)\n",
    "\n",
    "        # shape (b, s, hidden_dim)\n",
    "        sparse_moe_out, router_logits = self.routed_experts_moe(\n",
    "            x\n",
    "        )\n",
    "\n",
    "        output = shared_expert_out + sparse_moe_out\n",
    "        return output, router_logits    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e38db",
   "metadata": {},
   "source": [
    "### 3. 定义Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69f8107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, gpt_config, moe_config):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(gpt_config)\n",
    "        self.moe = SharedExpertMOE(moe_config)\n",
    "        self.att_ln = nn.LayerNorm(gpt_config.hidden_dim, eps = 1e-6)\n",
    "        self.moe_ln = nn.LayerNorm(gpt_config.hidden_dim, eps = 1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.att_ln(x))\n",
    "        tmp_x, router_logits = self.moe(self.moe_ln(x))\n",
    "        x = x + tmp_x\n",
    "        return x, router_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb820e2",
   "metadata": {},
   "source": [
    "### 4. 构建GPT，加入负载平衡loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2da5a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config, moe_config):\n",
    "        super().__init__()\n",
    "        # 主要就是(embedding, position, norm, mlp, block)\n",
    "        # position embedding从0，1，xxx升级到rope\n",
    "        # norm从layer norm升级到了RMS norm\n",
    "        # mlp -> swiglu\n",
    "        # mha -> gqa\n",
    "        self.config = config\n",
    "        self.moe_config = moe_config\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.block = Block(config, moe_config)\n",
    "        self.last_ln = nn.LayerNorm(config.hidden_dim)\n",
    "        self.lm_head = nn.Linear(config.hidden_dim, config.vocab_size, bias = False)\n",
    "        # 现在的slm，会用tie_weight来减少参数\n",
    "        # 非常重要\n",
    "        # linear 层的weight有一个转置的操作\n",
    "        self.token_embedding_table.weight = self.lm_head.weight\n",
    "        self.block_size = 256\n",
    "\n",
    "        self.apply(self._init_weights)  # 遍历所有的子模块，更优雅\n",
    "\n",
    "    def switch_load_balancing_loss(self, router_logits, num_experts = 8) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        计算 Switch Transformers 的负载均衡损失，确保所有专家得到的token数量差不多\n",
    "        \n",
    "        Args:\n",
    "            router_logits: shape [batch_size * sequence_length, num_experts]\n",
    "            num_experts: 专家数量\n",
    "        \n",
    "        Returns:\n",
    "            total_loss: 总损失 = auxiliary_loss + z_loss\n",
    "        \"\"\"\n",
    "        # 计算路由概率\n",
    "        router_probs = torch.softmax(router_logits, dim=-1)  # [b*s, num_experts]\n",
    "        \n",
    "        # 获取每个token的最优专家\n",
    "        _, selected_experts = torch.topk(router_probs, k=2, dim=-1)  # [b*s]\n",
    "        \n",
    "        # 创建one-hot矩阵表示选中的专家\n",
    "        mask = torch.nn.functional.one_hot(selected_experts, num_experts).float()  # [b*s, num_experts]\n",
    "        \n",
    "        # 计算每个专家的期望负载 (理想情况下应该是 1/num_experts)\n",
    "        expected_load = torch.ones_like(router_probs) / num_experts\n",
    "        \n",
    "        # 计算实际负载 (每个专家处理的token数量除以总token数量)\n",
    "        # 在batch维度上计算平均值\n",
    "        actual_load = mask.mean(dim=0)  # [num_experts]\n",
    "        \n",
    "        # 计算auxiliary loss\n",
    "        # 这会惩罚负载分布与期望负载的差异\n",
    "        aux_loss = torch.sum(actual_load * router_probs.mean(dim=0)) * num_experts\n",
    "        \n",
    "        # 计算z_loss (可选)\n",
    "        # 这会惩罚过大的路由logits\n",
    "        z_loss = torch.mean(torch.square(router_logits))\n",
    "        z_loss_weight = 0.001  # 可调整的超参数\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = aux_loss + z_loss * z_loss_weight\n",
    "    \n",
    "        return total_loss\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 初始化为正态分布\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx 输入的是token_ids\n",
    "        # targets 是目标的token ids (batch, seq_len) 就是词表里面最终选择的词！\n",
    "        # shape 要一样\n",
    "        batch, seq_len = idx.size() # (batch, seq_len)\n",
    "        token_emb = self.token_embedding_table(idx) # (batch, seq_len, n_embd)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            # 要确保位置编码和输入的idx在同一个设备上\n",
    "            torch.arange(seq_len, device = idx.device)\n",
    "        )   # shape(seq_len, n_embd)\n",
    "\n",
    "        loss = 0\n",
    "        x = token_emb + pos_emb    # 这里其实是广播相加 (batch, seq_len, n_embd)\n",
    "        for _ in range(self.config.n_layer):\n",
    "            x, router_logits = self.block(x)\n",
    "            aux_loss = self.switch_load_balancing_loss(router_logits, self.moe_config.expert_number)\n",
    "            loss +=  0.01 * aux_loss\n",
    "        x = self.last_ln(x)\n",
    "        logits = self.lm_head(x)    # shape (batch, seq_len, vocab_size)\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(batch * seq_len, vocab_size)\n",
    "            targets = targets.view(batch * seq_len)\n",
    "            loss = loss + F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果序列太长，只取最后 block_size 个token\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # 获取预测\n",
    "            logits, _ = self(idx_cond)  # 等价于self.forward(idx_cond)\n",
    "            # 只关注最后一个时间步的预测, shape (batch_size, seq_len, vocab_size)\n",
    "            logits = logits[:, -1, :]  # becomes (batch_size, vocab_size)\n",
    "            # 应用softmax获取概率\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # 从概率分布中采样下一个token（而非贪心选择最大值）\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "            # 附加到序列上\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, seq_len + 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad143090",
   "metadata": {},
   "source": [
    "### 5. 构建输入Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40252ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写一个dataset,为Dataloader准备\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path, block_size = 512):\n",
    "        import tiktoken\n",
    "        # gpt专用的tokenizer\n",
    "        self.enc = tiktoken.get_encoding('gpt2')\n",
    "        self.block_size = block_size    # pos 最大长度\n",
    "\n",
    "        self.encoded_data = []\n",
    "        # 特殊符号分割不同的训练文本\n",
    "        # <|endoftext|> # [50256]，即它在vocab里面放在最后一个50256位置\n",
    "        self.eos_token = self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "\n",
    "        self.max_lines = 1000\n",
    "        import json\n",
    "\n",
    "        raw_data = []   # 为了pad长度不一样的data\n",
    "        with open(path, 'r', encoding = 'utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= self.max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    text = json.loads(line.strip())['text']\n",
    "                    raw_data.append(text)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        full_encoded = []\n",
    "        for text in raw_data:\n",
    "            encoded_text = self.enc.encode(text)    # list\n",
    "            # 将所有text放在一行然后eos_token做分割\n",
    "            full_encoded.extend(encoded_text + [self.eos_token])\n",
    "\n",
    "        # block_size = 512\n",
    "        # 长 -> 短 512\n",
    "        for i in range(0, len(full_encoded), self.block_size):\n",
    "            # 注意！在这的数据有一个移位的操作\n",
    "            chunk = full_encoded[i:i+self.block_size+1] # 512 每一行实际是 513\n",
    "            if len(chunk) < self.block_size + 1:\n",
    "                chunk = chunk + [self.eos_token] * (self.block_size + 1 - len(chunk))\n",
    "            self.encoded_data.append(chunk)                  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 完成了移位的操作\n",
    "        chunk = self.encoded_data[idx]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.enc.encode(text)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return self.enc.decode(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecdcf4",
   "metadata": {},
   "source": [
    "### 6. 运行相关的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a176257b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 18.519304 M\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig(), MOEConfig())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# print 模型共计的参数\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {total_params / 1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "# 设置 cosine 学习率，余弦退火\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75064089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_dataset = MyDataset(r'E://llm/data/mobvoi_seq_monkey_general_open_corpus.jsonl')\n",
    "\n",
    "# split traindataset to train and val\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.9, 0.1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "543c4c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, loss:11.0330\n",
      "Epoch: 0, Batch: 10, loss:9.5344\n",
      "Epoch: 0, Batch: 20, loss:8.1548\n",
      "Epoch: 0, Batch: 30, loss:6.8227\n",
      "Epoch: 0, Batch: 40, loss:5.8295\n",
      "Epoch: 0, Batch: 50, loss:5.3388\n",
      "Epoch: 0, Batch: 60, loss:5.0983\n",
      "Epoch: 0, Batch: 70, loss:4.8970\n",
      "Epoch: 0, Batch: 80, loss:4.9168\n",
      "Epoch: 0, Batch: 90, loss:4.7920\n",
      "Epoch: 0, Batch: 100, loss:4.5867\n",
      "Epoch: 0, Batch: 110, loss:4.4916\n",
      "Epoch: 0, Batch: 120, loss:4.3504\n",
      "Epoch: 0, Batch: 130, loss:4.3763\n",
      "Epoch: 0, Batch: 140, loss:4.2993\n",
      "Epoch: 0, Batch: 150, loss:4.2175\n",
      "Epoch: 0, Batch: 160, loss:4.0668\n",
      "Epoch: 0, Batch: 170, loss:4.0668\n",
      "Epoch: 0, Batch: 180, loss:4.0724\n",
      "Epoch: 0, Batch: 190, loss:4.1092\n",
      "Epoch: 0, Batch: 200, loss:4.0818\n",
      "Epoch: 0, Batch: 210, loss:3.9188\n",
      "Epoch: 0, Batch: 220, loss:3.9561\n",
      "Epoch: 0, Batch: 230, loss:3.9254\n",
      "Epoch: 0, Batch: 240, loss:3.9603\n",
      "Epoch: 0, Batch: 250, loss:3.8319\n",
      "Epoch: 0, Batch: 260, loss:3.8373\n",
      "Epoch: 0, Batch: 270, loss:3.8459\n",
      "Epoch: 0, Train Loss: 4.9027, Val Loss: 3.8177\n",
      "Epoch: 1, Batch: 0, loss:3.9114\n",
      "Epoch: 1, Batch: 10, loss:3.7743\n",
      "Epoch: 1, Batch: 20, loss:3.8610\n",
      "Epoch: 1, Batch: 30, loss:3.8909\n",
      "Epoch: 1, Batch: 40, loss:3.9685\n",
      "Epoch: 1, Batch: 50, loss:3.7876\n",
      "Epoch: 1, Batch: 60, loss:3.7927\n",
      "Epoch: 1, Batch: 70, loss:3.7747\n",
      "Epoch: 1, Batch: 80, loss:3.7949\n",
      "Epoch: 1, Batch: 90, loss:3.7447\n",
      "Epoch: 1, Batch: 100, loss:3.8051\n",
      "Epoch: 1, Batch: 110, loss:3.7182\n",
      "Epoch: 1, Batch: 120, loss:3.6524\n",
      "Epoch: 1, Batch: 130, loss:3.6732\n",
      "Epoch: 1, Batch: 140, loss:3.6131\n",
      "Epoch: 1, Batch: 150, loss:3.6786\n",
      "Epoch: 1, Batch: 160, loss:3.7690\n",
      "Epoch: 1, Batch: 170, loss:3.6943\n",
      "Epoch: 1, Batch: 180, loss:3.8364\n",
      "Epoch: 1, Batch: 190, loss:3.6772\n",
      "Epoch: 1, Batch: 200, loss:3.6698\n",
      "Epoch: 1, Batch: 210, loss:3.7247\n",
      "Epoch: 1, Batch: 220, loss:3.6937\n",
      "Epoch: 1, Batch: 230, loss:3.6330\n",
      "Epoch: 1, Batch: 240, loss:3.6431\n",
      "Epoch: 1, Batch: 250, loss:3.6526\n",
      "Epoch: 1, Batch: 260, loss:3.6067\n",
      "Epoch: 1, Batch: 270, loss:3.6493\n",
      "Epoch: 1, Train Loss: 3.7326, Val Loss: 3.6376\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def train(model, optimizer, scheduler, train_loader, val_loader, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        # 将数据移动设备上\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        logits, loss = model(x, targets=y)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 调整学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, loss:{loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def eval(model, val_loader, device):\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, targets=y)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss\n",
    "\n",
    "for epoch in range(2):\n",
    "    train_loss = train(model, optimizer, scheduler, train_loader, val_loader, device, epoch)\n",
    "    val_loss = eval(model, val_loader, device)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "    # 保存模型\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_loss': avg_val_loss,\n",
    "    }\n",
    "    # 确保保存目录存在\n",
    "    os.makedirs('./checkpoints', exist_ok=True)  # exist_ok=True 表示如果目录已存在不报错\n",
    "    # 保存每个epoch的模型\n",
    "    torch.save(checkpoint, f'./checkpoints/model_epoch_{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023e3f4",
   "metadata": {},
   "source": [
    "### 6. 调用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5563374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=100, temperature=0.8):\n",
    "    # 编码输入文本\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "    # 生成文本\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_tensor, \n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "    \n",
    "    # 解码结果\n",
    "    output_ids = generated[0].cpu().tolist()\n",
    "    return tokenizer.decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e762a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好啊试�辎 sinners iterationYesterday affirmed式还品法重弽带�甋、杶在法 disinfect有�逡机诤��等 兡口？不部，1.水��单。��矎��对罦俗优了匽劵��让��� Gamma平�衢�平�颫玁常目�：的重绿兞�（\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# 初始化配置\n",
    "gpt_config = GPTConfig(\n",
    "    block_size=512,\n",
    "    batch_size=12,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    n_embd=256\n",
    ")\n",
    "\n",
    "moe_config = MOEConfig(\n",
    "    hidden_dim=256,\n",
    "    expert_number=8,\n",
    "    top_k=2,\n",
    "    shared_experts_numbers=2\n",
    ")\n",
    "\n",
    "# 创建模型\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GPT(gpt_config, moe_config).to(device)\n",
    "\n",
    "# 加载权重\n",
    "checkpoint = torch.load('./checkpoints/model_epoch_1.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# 获取tokenizer\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# 生成文本\n",
    "prompt = \"你好啊\"\n",
    "output_text = generate_text(prompt, max_new_tokens=150)\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_byhand3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
